# -*- coding: utf-8 -*-
"""zindi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QbIo0Usz5jBxxTVFmOnol2IiTyH9VmLH
"""

import getpass

OPENAI_API_KEY=getpass.getpass()

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.vectorstores import FAISS
#from langchain.chat_models import ChatOpenAI
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain_community.llms import Ollama
# For openai key
import os
os.environ["OPENAI_API_KEY"] =OPENAI_API_KEY

# Montage du drive
# from google.colab import drive
# drive.mount('/gdrive')
pdf = "./recueil.pdf"



from langchain_community.document_loaders import WebBaseLoader
#loader = WebBaseLoader("https://docs.smith.langchain.com/user_guide")
#loader = WebBaseLoader("https://service-public.bj/public/services/e-services")

#docs = loader.load()

# load a PDF
loader = PyPDFLoader(pdf)
documents = loader.load()
# Split text
text = RecursiveCharacterTextSplitter(chunk_size= 1000 , 
    chunk_overlap= 0 ,).split_documents(documents)

# Load embedding model
embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5",
encode_kwargs={"normalize_embeddings": True})
# Create a vectorstore
vectorstore = FAISS.from_documents(text, embeddings)

# Save the documents and embeddings
vectorstore.save_local("vectorstore.db")



# Créer un retriever
retriever = vectorstore.as_retriever()

# !pip install -U transformers
# from transformers import AutoTokenizer, AutoModelForCausalLM
# model = AutoModelForCausalLM.from_pretrained("google/gemma-7b")

# Charger le llm

# llm = Ollama(
#         model="llama2",
#         base_url="http://localhost:11434",
#         verbose=True,
#     )
llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=OPENAI_API_KEY)

# Définir le modèle d'invite
template= """
Vous êtes un assistant pour les tâches de réponse aux questions.
Utilisez le contexte fourni uniquement pour répondre à la question suivante :

<context >
{context}
</context>

Question : {input}
"""

# Créer un modèle d'invite
prompt = ChatPromptTemplate.from_template(template)

# Créer une chaîne
doc_chain = create_stuff_documents_chain(llm, prompt)
chain = create_retrieval_chain(retriever, doc_chain)



# User query
response = chain.invoke({"input": "un petit résumé"})

# Get the Answer only
response['answer']

# for i in range(5):
#   response = chain.invoke({"input": "un petit résumé du document"})
#   print(response['answer'])



# # split pages content
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # create the parent documents - The big chunks
# parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)

# # create the child documents - The small chunks
# child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

# # The storage layer for the parent chunks
# from langchain.storage import InMemoryStore
# store = InMemoryStore()

# !pip install chromadb
# # create vectorstore using Chromadb
# from langchain.vectorstores import Chroma
# vectorstore = Chroma(collection_name="split_parents", embedding_function=embeddings)

# # create retriever
# from langchain.retrievers import ParentDocumentRetriever
# retriever = ParentDocumentRetriever(
#     vectorstore=vectorstore,
#     docstore=store,
#     child_splitter=child_splitter,
#     parent_splitter=parent_splitter,
# )

# # add documents to vectorstore
# retriever.add_documents(documents)

# doc_chain = create_stuff_documents_chain(llm, prompt)
# chain = create_retrieval_chain(retriever, doc_chain)

# # User query
# response = chain.invoke({"input": "Un petit résumé du document"})

# # Get the Answer only
# response['answer']

